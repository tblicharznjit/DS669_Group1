{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMP4B4iF1Pm1"
      },
      "outputs": [],
      "source": [
        "# train_runner_multi.py\n",
        "import os, pickle, numpy as np, torch, matplotlib.pyplot as plt, pandas as pd\n",
        "from matplotlib import cm, colors\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "from ID3QNE_deepQnet import Dist_DQN\n",
        "\n",
        "with open(\"requiredFile.pkl\", \"rb\") as f:\n",
        "    D = pickle.load(f)\n",
        "\n",
        "Xtr = D[\"X_train\"].to_numpy()\n",
        "Xte = D[\"X_test\"].to_numpy()\n",
        "Xnext_tr = D[\"Xnext_train\"].to_numpy()\n",
        "ytr = D[\"y_train\"]\n",
        "yte = D[\"y_test\"]\n",
        "Atr = D[\"Action_train\"]\n",
        "n_actions = D.get(\"nbins\", 25)\n",
        "\n",
        "exps = [\n",
        "    {\"name\": \"exp1_baseline\",  \"seed\": 42, \"gamma\": 0.99, \"lr\": 1e-5, \"epochs\": 100},\n",
        "    {\"name\": \"exp2_gamma095\",  \"seed\": 42, \"gamma\": 0.95, \"lr\": 1e-5, \"epochs\": 100},\n",
        "    {\"name\": \"exp3_higher_lr\", \"seed\": 42, \"gamma\": 0.99, \"lr\": 3e-5, \"epochs\": 100},\n",
        "    {\"name\": \"exp4_seed7\",     \"seed\": 7,  \"gamma\": 0.99, \"lr\": 1e-5, \"epochs\": 100},\n",
        "]\n",
        "\n",
        "def ensure_dir(p): os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def plot_training_loss(log_path, out_png, title):\n",
        "    losses = []\n",
        "    with open(log_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            if \"Loss:\" in line:\n",
        "                try: losses.append(float(line.strip().split(\"Loss:\")[-1]))\n",
        "                except: pass\n",
        "    plt.close(\"all\")\n",
        "    plt.figure(figsize=(6,4), constrained_layout=True)\n",
        "    if losses: plt.plot(losses)\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(title); plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout(); plt.savefig(out_png, dpi=200); plt.close()\n",
        "\n",
        "def plot_train_action_and_survival(A_train, y_train, out_dir, exp_name):\n",
        "    plt.close(\"all\")\n",
        "    plt.figure(figsize=(6,4), constrained_layout=True)\n",
        "    counts = np.bincount(A_train, minlength=n_actions)\n",
        "    plt.bar(range(len(counts)), counts)\n",
        "    plt.xlabel(\"Historical Action (0..24)\"); plt.ylabel(\"Count\")\n",
        "    plt.title(f\"{exp_name} – Training Action Distribution\")\n",
        "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
        "    out1 = os.path.join(out_dir, f\"{exp_name}_train_action_distribution.png\")\n",
        "    plt.savefig(out1, dpi=200); plt.close()\n",
        "\n",
        "    df = pd.DataFrame({\"action\": A_train, \"surv\": (y_train == 0).astype(int)})\n",
        "    by = df.groupby(\"action\")[\"surv\"].mean()\n",
        "    plt.figure(figsize=(6,4), constrained_layout=True)\n",
        "    by.plot(kind=\"bar\"); plt.ylim(0,1)\n",
        "    plt.xlabel(\"Historical Action (0..24)\"); plt.ylabel(\"Survival Rate\")\n",
        "    plt.title(f\"{exp_name} – Training Survival by Action\")\n",
        "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
        "    out2 = os.path.join(out_dir, f\"{exp_name}_train_survival_by_action.png\")\n",
        "    plt.tight_layout(); plt.savefig(out2, dpi=200); plt.close()\n",
        "\n",
        "def policy_figure(q_net, Xtest_np, out_png, exp_name):\n",
        "    with torch.no_grad():\n",
        "        q_vals = q_net(torch.tensor(Xtest_np, dtype=torch.float32))\n",
        "        actions_pred = q_vals.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "    fluid = actions_pred % 5; vaso = actions_pred // 5; N = len(actions_pred)\n",
        "    fluid_prop = np.bincount(fluid, minlength=5).astype(float) / max(N,1)\n",
        "    vaso_prop  = np.bincount(vaso,  minlength=5).astype(float) / max(N,1)\n",
        "    heat = np.zeros((5,5), float)\n",
        "    for f,v in zip(fluid, vaso): heat[v,f]+=1.0\n",
        "    heat_prop = heat / max(N,1)\n",
        "\n",
        "    plt.close(\"all\")\n",
        "    fig = plt.figure(figsize=(15, 4.8), constrained_layout=True)\n",
        "\n",
        "    ax1 = fig.add_subplot(1,3,1)\n",
        "    ax1.bar(range(5), fluid_prop)\n",
        "    ax1.set_xticks(range(5)); ax1.set_xticklabels([f\"F{i}\" for i in range(5)])\n",
        "    ax1.set_ylim(0,1); ax1.set_xlabel(\"Fluid Bin (0..4)\", fontsize=9)\n",
        "    ax1.set_ylabel(\"Proportion\", fontsize=9); ax1.tick_params(labelsize=9)\n",
        "    ax1.set_title(f\"{exp_name}\\nFluid Distribution (Test, Proportion)\", pad=10, fontsize=11)\n",
        "    ax1.grid(True, axis=\"y\", alpha=0.3)\n",
        "\n",
        "    ax2 = fig.add_subplot(1,3,2)\n",
        "    ax2.bar(range(5), vaso_prop)\n",
        "    ax2.set_xticks(range(5)); ax2.set_xticklabels([f\"V{i}\" for i in range(5)])\n",
        "    ax2.set_ylim(0,1); ax2.set_xlabel(\"Vasopressor Bin (0..4)\", fontsize=9)\n",
        "    ax2.set_ylabel(\"Proportion\", fontsize=9); ax2.tick_params(labelsize=9)\n",
        "    ax2.set_title(f\"{exp_name}\\nVasopressor Distribution (Test, Proportion)\", pad=10, fontsize=11)\n",
        "    ax2.grid(True, axis=\"y\", alpha=0.3)\n",
        "\n",
        "    ax3 = fig.add_subplot(1,3,3, projection=\"3d\")\n",
        "    _x = np.arange(5); _y = np.arange(5)\n",
        "    _xx,_yy = np.meshgrid(_x,_y)\n",
        "    x = _xx.ravel(); y = _yy.ravel(); z = np.zeros_like(x, dtype=float)\n",
        "    dx = np.full_like(x, 0.6, float); dy = np.full_like(y, 0.6, float)\n",
        "    dz = heat_prop.ravel()\n",
        "    cmap = cm.get_cmap(\"coolwarm\")\n",
        "    norm = colors.Normalize(vmin=dz.min(), vmax=dz.max() if dz.max()>0 else 1.0)\n",
        "    colors_bar = cmap(norm(dz))\n",
        "    ax3.bar3d(x-0.3, y-0.3, z, dx, dy, dz, color=colors_bar, shade=True)\n",
        "    ax3.set_xticks(range(5)); ax3.set_xticklabels([f\"F{i}\" for i in range(5)], fontsize=8)\n",
        "    ax3.set_yticks(range(5)); ax3.set_yticklabels([f\"V{i}\" for i in range(5)], fontsize=8)\n",
        "    ax3.set_zlabel(\"Proportion\", fontsize=9)\n",
        "    ax3.set_xlabel(\"Fluid Bin\", fontsize=9); ax3.set_ylabel(\"Vasopressor Bin\", fontsize=9)\n",
        "    ax3.set_title(f\"{exp_name}\\n5×5 Action Heatmap (Test, Proportion)\", pad=10, fontsize=11)\n",
        "    mappable = cm.ScalarMappable(norm=norm, cmap=cmap); mappable.set_array([])\n",
        "    cb = fig.colorbar(mappable, ax=ax3, fraction=0.046, pad=0.08)\n",
        "    cb.set_label(\"Proportion\", fontsize=9)\n",
        "\n",
        "    fig.savefig(out_png, dpi=220); plt.close(fig)\n",
        "\n",
        "def summarize_metrics(out_csv, rows):\n",
        "    df = pd.DataFrame(rows)\n",
        "    if os.path.exists(out_csv):\n",
        "        base = pd.read_csv(out_csv); df = pd.concat([base, df], ignore_index=True)\n",
        "    df.to_csv(out_csv, index=False)\n",
        "\n",
        "summary_rows = []\n",
        "for cfg in exps:\n",
        "    name = cfg[\"name\"]; epochs = cfg[\"epochs\"]\n",
        "    outdir = os.path.join(\"runs\", name); ensure_dir(outdir)\n",
        "    print(f\"\\n=== Running {name} (epochs={epochs}, gamma={cfg['gamma']}, lr={cfg['lr']}, seed={cfg['seed']}) ===\")\n",
        "\n",
        "    model = Dist_DQN(state_dim=Xtr.shape[1], num_actions=n_actions,\n",
        "                     gamma=cfg[\"gamma\"], lr=cfg[\"lr\"], seed=cfg[\"seed\"])\n",
        "\n",
        "    log_path = os.path.join(outdir, f\"{name}_training_log.txt\")\n",
        "    open(log_path, \"w\").close()\n",
        "    batchs = {\"state\": Xtr, \"next_state\": Xnext_tr, \"action\": Atr, \"reward\": np.where(ytr==0,24.0,-24.0)}\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        loss = model.train_model(batchs, ep)\n",
        "        print(f\"{name} | Epoch {ep+1}/{epochs} | Loss: {loss:.4f}\")\n",
        "        with open(log_path, \"a\") as lf:\n",
        "            lf.write(f\"Epoch {ep+1} | Loss: {loss:.4f}\\n\")\n",
        "\n",
        "    # save model\n",
        "    model_path = os.path.join(outdir, f\"{name}_model.pt\")\n",
        "    torch.save(model.q_net.state_dict(), model_path)\n",
        "\n",
        "    # plots\n",
        "    policy_png = os.path.join(outdir, f\"{name}_policy_combined.png\")\n",
        "    policy_figure(model.q_net, Xte, policy_png, name)\n",
        "\n",
        "    loss_png = os.path.join(outdir, f\"{name}_training_loss.png\")\n",
        "    plot_training_loss(log_path, loss_png, f\"{name} – Training Loss Over Time\")\n",
        "\n",
        "    plot_train_action_and_survival(Atr, ytr, outdir, name)\n",
        "\n",
        "    # metrics\n",
        "    sr_train = (ytr == 0).mean()\n",
        "    sr_test  = (yte == 0).mean()\n",
        "    er_train = np.where(ytr == 0, 24.0, -24.0).mean()\n",
        "    er_test  = np.where(yte == 0, 24.0, -24.0).mean()\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"experiment\": name, \"epochs\": epochs,\n",
        "        \"gamma\": cfg[\"gamma\"], \"lr\": cfg[\"lr\"], \"seed\": cfg[\"seed\"],\n",
        "        \"survival_train\": sr_train, \"survival_test\": sr_test,\n",
        "        \"exp_return_train\": er_train, \"exp_return_test\": er_test\n",
        "    })\n",
        "\n",
        "summarize_metrics(\"runs/summary_metrics.csv\", summary_rows)\n",
        "print(\"\\nAll 4 experiments complete. Results in runs/exp*/ and runs/summary_metrics.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## My approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ID3QNE_deepQnet import Dist_DQN\n",
        "from ID3QNE_evaluate import do_test\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'mps'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train, validation, and test .pkl files created.\n"
          ]
        }
      ],
      "source": [
        "with open('requiredFile.pkl', 'rb') as f:\n",
        "    MIMICtable = pickle.load(f)\n",
        "\n",
        "# Extract data\n",
        "X = MIMICtable['X']\n",
        "Xnext = MIMICtable['Xnext']\n",
        "Action = MIMICtable['Action']\n",
        "ActionNext = MIMICtable['ActionNext']\n",
        "Reward = MIMICtable['Reward']\n",
        "Done = MIMICtable['Done']\n",
        "Bloc = MIMICtable['Bloc']\n",
        "SOFA = MIMICtable['SOFA']\n",
        "Y90 = np.array([1 if r == -24 else 0 for r in Reward[Done == 1]])  # Infer 90D_Mortality from final reward\n",
        "\n",
        "# Split by unique Bloc (patient) IDs\n",
        "unique_blocs = np.unique(Bloc)\n",
        "train_blocs, test_val_blocs = train_test_split(unique_blocs, test_size=0.2, random_state=42)\n",
        "val_blocs, test_blocs = train_test_split(test_val_blocs, test_size=0.5, random_state=42)\n",
        "\n",
        "# Create masks\n",
        "train_mask = np.isin(Bloc, train_blocs)\n",
        "val_mask = np.isin(Bloc, val_blocs)\n",
        "test_mask = np.isin(Bloc, test_blocs)\n",
        "\n",
        "# Create datasets\n",
        "train_data = {\n",
        "    'X': X[train_mask],\n",
        "    'Xnext': Xnext[train_mask],\n",
        "    'Action': Action[train_mask],\n",
        "    'ActionNext': ActionNext[train_mask],\n",
        "    'Reward': Reward[train_mask],\n",
        "    'Done': Done[train_mask],\n",
        "    'Bloc': Bloc[train_mask],\n",
        "    'SOFA': SOFA[train_mask]\n",
        "}\n",
        "\n",
        "val_data = {\n",
        "    'X': X[val_mask],\n",
        "    'Xnext': Xnext[val_mask],\n",
        "    'Action': Action[val_mask],\n",
        "    'ActionNext': ActionNext[val_mask],\n",
        "    'Reward': Reward[val_mask],\n",
        "    'Done': Done[val_mask],\n",
        "    'Bloc': Bloc[val_mask],\n",
        "    'SOFA': SOFA[val_mask]\n",
        "}\n",
        "\n",
        "test_data = {\n",
        "    'X': X[test_mask],\n",
        "    'Xnext': Xnext[test_mask],\n",
        "    'Action': Action[test_mask],\n",
        "    'ActionNext': ActionNext[test_mask],\n",
        "    'Reward': Reward[test_mask],\n",
        "    'Done': Done[test_mask],\n",
        "    'Bloc': Bloc[test_mask],\n",
        "    'SOFA': SOFA[test_mask]\n",
        "}\n",
        "\n",
        "# Save to .pkl files\n",
        "with open('train_data.pkl', 'wb') as f:\n",
        "    pickle.dump(train_data, f)\n",
        "with open('val_data.pkl', 'wb') as f:\n",
        "    pickle.dump(val_data, f)\n",
        "with open('test_data.pkl', 'wb') as f:\n",
        "    pickle.dump(test_data, f)\n",
        "\n",
        "print(\"Train, validation, and test .pkl files created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('train_data.pkl', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "with open('val_data.pkl', 'rb') as f:\n",
        "    val_data = pickle.load(f)\n",
        "with open('test_data.pkl', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "# Prepare training batch\n",
        "train_batch = (\n",
        "    train_data['X'],\n",
        "    train_data['Xnext'],\n",
        "    train_data['Action'],\n",
        "    train_data['ActionNext'],\n",
        "    train_data['Reward'],\n",
        "    train_data['Done'],\n",
        "    train_data['Bloc'],\n",
        "    train_data['SOFA']\n",
        ")\n",
        "\n",
        "# Prepare validation batch\n",
        "val_batch = (\n",
        "    val_data['X'],\n",
        "    val_data['Xnext'],\n",
        "    val_data['Action'],\n",
        "    val_data['ActionNext'],\n",
        "    val_data['Reward'],\n",
        "    val_data['Done'],\n",
        "    val_data['Bloc'],\n",
        "    val_data['SOFA']\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "state_dim = train_data['X'].shape[1] \n",
        "num_actions = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Dist_DQN(state_dim=state_dim, num_actions=num_actions, device=device, gamma=0.999, tau=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Batch: 0, Average Loss: 36.8125\n",
            "Epoch: 0, Batch: 25, Average Loss: 2.9972785115242004\n",
            "Epoch: 0, Batch: 50, Average Loss: 2.3319448513143204\n",
            "Epoch: 0, Batch: 75, Average Loss: 2.102175770621551\n",
            "Epoch: 0, Batch: 100, Average Loss: 1.9882380584679027\n",
            "Epoch: 0, Validation Loss: 1.635301947593689\n",
            "Saved best model at epoch 0\n",
            "Epoch: 1, Batch: 0, Average Loss: 1.6094462871551514\n",
            "Epoch: 1, Batch: 25, Average Loss: 1.6385688277391286\n",
            "Epoch: 1, Batch: 50, Average Loss: 1.6375076069551355\n",
            "Epoch: 1, Batch: 75, Average Loss: 1.6352406740188599\n",
            "Epoch: 1, Batch: 100, Average Loss: 1.6362868842512075\n",
            "Epoch: 1, Validation Loss: 1.6337321996688843\n",
            "Saved best model at epoch 1\n",
            "Epoch: 2, Batch: 0, Average Loss: 1.6077905893325806\n",
            "Epoch: 2, Batch: 25, Average Loss: 1.6365315363957331\n",
            "Epoch: 2, Batch: 50, Average Loss: 1.6355922923368567\n",
            "Epoch: 2, Batch: 75, Average Loss: 1.6333865664507214\n",
            "Epoch: 2, Batch: 100, Average Loss: 1.6344632004747297\n",
            "Epoch: 2, Validation Loss: 1.6322962045669556\n",
            "Saved best model at epoch 2\n",
            "Epoch: 3, Batch: 0, Average Loss: 1.6067771911621094\n",
            "Epoch: 3, Batch: 25, Average Loss: 1.6355472482167757\n",
            "Epoch: 3, Batch: 50, Average Loss: 1.6346423298704857\n",
            "Epoch: 3, Batch: 75, Average Loss: 1.632450265319724\n",
            "Epoch: 3, Batch: 100, Average Loss: 1.6335439752824237\n",
            "Epoch: 3, Validation Loss: 1.630660891532898\n",
            "Saved best model at epoch 3\n",
            "Epoch: 4, Batch: 0, Average Loss: 1.6063811779022217\n",
            "Epoch: 4, Batch: 25, Average Loss: 1.635160010594588\n",
            "Epoch: 4, Batch: 50, Average Loss: 1.6342861652374268\n",
            "Epoch: 4, Batch: 75, Average Loss: 1.6321154048568325\n",
            "Epoch: 4, Batch: 100, Average Loss: 1.633240783568656\n",
            "Epoch: 4, Validation Loss: 1.6289925575256348\n",
            "Saved best model at epoch 4\n",
            "Epoch: 5, Batch: 0, Average Loss: 1.6065657138824463\n",
            "Epoch: 5, Batch: 25, Average Loss: 1.6343537248097932\n",
            "Epoch: 5, Batch: 50, Average Loss: 1.6333926051270728\n",
            "Epoch: 5, Batch: 75, Average Loss: 1.6312069469376613\n",
            "Epoch: 5, Batch: 100, Average Loss: 1.632339367772093\n",
            "Epoch: 5, Validation Loss: 1.6279125213623047\n",
            "Saved best model at epoch 5\n",
            "Epoch: 6, Batch: 0, Average Loss: 1.6061111688613892\n",
            "Epoch: 6, Batch: 25, Average Loss: 1.6349956668340242\n",
            "Epoch: 6, Batch: 50, Average Loss: 1.6341728790133607\n",
            "Epoch: 6, Batch: 75, Average Loss: 1.632041705282111\n",
            "Epoch: 6, Batch: 100, Average Loss: 1.6332193353388569\n",
            "Epoch: 6, Validation Loss: 1.626192569732666\n",
            "Saved best model at epoch 6\n",
            "Epoch: 7, Batch: 0, Average Loss: 1.6071834564208984\n",
            "Epoch: 7, Batch: 25, Average Loss: 1.6360733830011809\n",
            "Epoch: 7, Batch: 50, Average Loss: 1.6352427520003974\n",
            "Epoch: 7, Batch: 75, Average Loss: 1.6331152053255784\n",
            "Epoch: 7, Batch: 100, Average Loss: 1.6343096990396482\n",
            "Epoch: 7, Validation Loss: 1.6244516372680664\n",
            "Saved best model at epoch 7\n",
            "Epoch: 8, Batch: 0, Average Loss: 1.6082789897918701\n",
            "Epoch: 8, Batch: 25, Average Loss: 1.637146688424624\n",
            "Epoch: 8, Batch: 50, Average Loss: 1.636301045324288\n",
            "Epoch: 8, Batch: 75, Average Loss: 1.6341732928627415\n",
            "Epoch: 8, Batch: 100, Average Loss: 1.63538129022806\n",
            "Epoch: 8, Validation Loss: 1.6226766109466553\n",
            "Saved best model at epoch 8\n",
            "Epoch: 9, Batch: 0, Average Loss: 1.6092897653579712\n",
            "Epoch: 9, Batch: 25, Average Loss: 1.6381135078576894\n",
            "Epoch: 9, Batch: 50, Average Loss: 1.6372499933429794\n",
            "Epoch: 9, Batch: 75, Average Loss: 1.6351198867747658\n",
            "Epoch: 9, Batch: 100, Average Loss: 1.6363395145623991\n",
            "Epoch: 9, Validation Loss: 1.620869517326355\n",
            "Saved best model at epoch 9\n",
            "Epoch: 10, Batch: 0, Average Loss: 1.6101423501968384\n",
            "Epoch: 10, Batch: 25, Average Loss: 1.6389949413446279\n",
            "Epoch: 10, Batch: 50, Average Loss: 1.638111102814768\n",
            "Epoch: 10, Batch: 75, Average Loss: 1.6359772337110419\n",
            "Epoch: 10, Batch: 100, Average Loss: 1.6372072318992992\n",
            "Epoch: 10, Validation Loss: 1.619022250175476\n",
            "Saved best model at epoch 10\n",
            "Epoch: 11, Batch: 0, Average Loss: 1.611053705215454\n",
            "Epoch: 11, Batch: 25, Average Loss: 1.6399074563613305\n",
            "Epoch: 11, Batch: 50, Average Loss: 1.6389992166967953\n",
            "Epoch: 11, Batch: 75, Average Loss: 1.6368603314224042\n",
            "Epoch: 11, Batch: 100, Average Loss: 1.6381001519684744\n",
            "Epoch: 11, Validation Loss: 1.617079257965088\n",
            "Saved best model at epoch 11\n",
            "Epoch: 12, Batch: 0, Average Loss: 1.6117970943450928\n",
            "Epoch: 12, Batch: 25, Average Loss: 1.6405579585295458\n",
            "Epoch: 12, Batch: 50, Average Loss: 1.6396284524132223\n",
            "Epoch: 12, Batch: 75, Average Loss: 1.6374848139913458\n",
            "Epoch: 12, Batch: 100, Average Loss: 1.6387334837771879\n",
            "Epoch: 12, Validation Loss: 1.615067720413208\n",
            "Saved best model at epoch 12\n",
            "Epoch: 13, Batch: 0, Average Loss: 1.612278699874878\n",
            "Epoch: 13, Batch: 25, Average Loss: 1.6409392631970918\n",
            "Epoch: 13, Batch: 50, Average Loss: 1.6399922277413161\n",
            "Epoch: 13, Batch: 75, Average Loss: 1.6378435665055324\n",
            "Epoch: 13, Batch: 100, Average Loss: 1.639100219943736\n",
            "Epoch: 13, Validation Loss: 1.6130125522613525\n",
            "Saved best model at epoch 13\n",
            "Epoch: 14, Batch: 0, Average Loss: 1.6125049591064453\n",
            "Epoch: 14, Batch: 25, Average Loss: 1.6410514162136958\n",
            "Epoch: 14, Batch: 50, Average Loss: 1.6400889789356905\n",
            "Epoch: 14, Batch: 75, Average Loss: 1.6379347082815672\n",
            "Epoch: 14, Batch: 100, Average Loss: 1.639198351614546\n",
            "Epoch: 14, Validation Loss: 1.6108934879302979\n",
            "Saved best model at epoch 14\n",
            "Epoch: 15, Batch: 0, Average Loss: 1.6124597787857056\n",
            "Epoch: 15, Batch: 25, Average Loss: 1.6408856281867394\n",
            "Epoch: 15, Batch: 50, Average Loss: 1.6399115417517869\n",
            "Epoch: 15, Batch: 75, Average Loss: 1.6377512765558142\n",
            "Epoch: 15, Batch: 100, Average Loss: 1.6390210165835843\n",
            "Epoch: 15, Validation Loss: 1.6087061166763306\n",
            "Saved best model at epoch 15\n",
            "Epoch: 16, Batch: 0, Average Loss: 1.6121498346328735\n",
            "Epoch: 16, Batch: 25, Average Loss: 1.640458460037525\n",
            "Epoch: 16, Batch: 50, Average Loss: 1.6394750277201335\n",
            "Epoch: 16, Batch: 75, Average Loss: 1.6373083826742674\n",
            "Epoch: 16, Batch: 100, Average Loss: 1.6385836896329824\n",
            "Epoch: 16, Validation Loss: 1.606465220451355\n",
            "Saved best model at epoch 16\n",
            "Epoch: 17, Batch: 0, Average Loss: 1.6115903854370117\n",
            "Epoch: 17, Batch: 25, Average Loss: 1.639771649470696\n",
            "Epoch: 17, Batch: 50, Average Loss: 1.6387827115900375\n",
            "Epoch: 17, Batch: 75, Average Loss: 1.6366091903887297\n",
            "Epoch: 17, Batch: 100, Average Loss: 1.6378892034587293\n",
            "Epoch: 17, Validation Loss: 1.6041673421859741\n",
            "Saved best model at epoch 17\n",
            "Epoch: 18, Batch: 0, Average Loss: 1.610802412033081\n",
            "Epoch: 18, Batch: 25, Average Loss: 1.6388645768165588\n",
            "Epoch: 18, Batch: 50, Average Loss: 1.6378732872944253\n",
            "Epoch: 18, Batch: 75, Average Loss: 1.6356925980040902\n",
            "Epoch: 18, Batch: 100, Average Loss: 1.636976941977397\n",
            "Epoch: 18, Validation Loss: 1.601798415184021\n",
            "Saved best model at epoch 18\n",
            "Epoch: 19, Batch: 0, Average Loss: 1.6097915172576904\n",
            "Epoch: 19, Batch: 25, Average Loss: 1.6377174854278564\n",
            "Epoch: 19, Batch: 50, Average Loss: 1.6367247572132186\n",
            "Epoch: 19, Batch: 75, Average Loss: 1.6345363823991073\n",
            "Epoch: 19, Batch: 100, Average Loss: 1.6358245670205296\n",
            "Epoch: 19, Validation Loss: 1.5993772745132446\n",
            "Saved best model at epoch 19\n",
            "Epoch: 20, Batch: 0, Average Loss: 1.60854971408844\n",
            "Epoch: 20, Batch: 25, Average Loss: 1.6363473397034864\n",
            "Epoch: 20, Batch: 50, Average Loss: 1.6353561878204346\n",
            "Epoch: 20, Batch: 75, Average Loss: 1.6331598617528613\n",
            "Epoch: 20, Batch: 100, Average Loss: 1.6344513952141941\n",
            "Epoch: 20, Validation Loss: 1.5968924760818481\n",
            "Saved best model at epoch 20\n",
            "Epoch: 21, Batch: 0, Average Loss: 1.607116460800171\n",
            "Epoch: 21, Batch: 25, Average Loss: 1.634785111133869\n",
            "Epoch: 21, Batch: 50, Average Loss: 1.6337987067652684\n",
            "Epoch: 21, Batch: 75, Average Loss: 1.6315940602829582\n",
            "Epoch: 21, Batch: 100, Average Loss: 1.6328887797818326\n",
            "Epoch: 21, Validation Loss: 1.5943447351455688\n",
            "Saved best model at epoch 21\n",
            "Epoch: 22, Batch: 0, Average Loss: 1.6054966449737549\n",
            "Epoch: 22, Batch: 25, Average Loss: 1.6330407766195445\n",
            "Epoch: 22, Batch: 50, Average Loss: 1.6320610490499758\n",
            "Epoch: 22, Batch: 75, Average Loss: 1.6298480896573317\n",
            "Epoch: 22, Batch: 100, Average Loss: 1.6311456118479815\n",
            "Epoch: 22, Validation Loss: 1.59175443649292\n",
            "Saved best model at epoch 22\n",
            "Epoch: 23, Batch: 0, Average Loss: 1.6037181615829468\n",
            "Epoch: 23, Batch: 25, Average Loss: 1.6311351794462938\n",
            "Epoch: 23, Batch: 50, Average Loss: 1.6301639757904351\n",
            "Epoch: 23, Batch: 75, Average Loss: 1.6279422170237492\n",
            "Epoch: 23, Batch: 100, Average Loss: 1.6292422998069536\n",
            "Epoch: 23, Validation Loss: 1.589104413986206\n",
            "Saved best model at epoch 23\n",
            "Epoch: 24, Batch: 0, Average Loss: 1.6017844676971436\n",
            "Epoch: 24, Batch: 25, Average Loss: 1.629084119429955\n",
            "Epoch: 24, Batch: 50, Average Loss: 1.628124015004027\n",
            "Epoch: 24, Batch: 75, Average Loss: 1.6258936916526996\n",
            "Epoch: 24, Batch: 100, Average Loss: 1.627196232871254\n",
            "Epoch: 24, Validation Loss: 1.5864169597625732\n",
            "Saved best model at epoch 24\n",
            "Epoch: 25, Batch: 0, Average Loss: 1.599724531173706\n",
            "Epoch: 25, Batch: 25, Average Loss: 1.6269071514789875\n",
            "Epoch: 25, Batch: 50, Average Loss: 1.625959365975623\n",
            "Epoch: 25, Batch: 75, Average Loss: 1.6237201251481708\n",
            "Epoch: 25, Batch: 100, Average Loss: 1.6250249796574658\n",
            "Epoch: 25, Validation Loss: 1.5836845636367798\n",
            "Saved best model at epoch 25\n",
            "Epoch: 26, Batch: 0, Average Loss: 1.5975513458251953\n",
            "Epoch: 26, Batch: 25, Average Loss: 1.6246181038709788\n",
            "Epoch: 26, Batch: 50, Average Loss: 1.623684565226237\n",
            "Epoch: 26, Batch: 75, Average Loss: 1.6214364280826168\n",
            "Epoch: 26, Batch: 100, Average Loss: 1.6227434826369334\n",
            "Epoch: 26, Validation Loss: 1.5809106826782227\n",
            "Saved best model at epoch 26\n",
            "Epoch: 27, Batch: 0, Average Loss: 1.5952727794647217\n",
            "Epoch: 27, Batch: 25, Average Loss: 1.6222234184925373\n",
            "Epoch: 27, Batch: 50, Average Loss: 1.6213053488263898\n",
            "Epoch: 27, Batch: 75, Average Loss: 1.619048341324455\n",
            "Epoch: 27, Batch: 100, Average Loss: 1.6203575193291844\n",
            "Epoch: 27, Validation Loss: 1.5780996084213257\n",
            "Saved best model at epoch 27\n",
            "Epoch: 28, Batch: 0, Average Loss: 1.5929005146026611\n",
            "Epoch: 28, Batch: 25, Average Loss: 1.6197366210130544\n",
            "Epoch: 28, Batch: 50, Average Loss: 1.6188355520659803\n",
            "Epoch: 28, Batch: 75, Average Loss: 1.6165694359101748\n",
            "Epoch: 28, Batch: 100, Average Loss: 1.6178805686459683\n",
            "Epoch: 28, Validation Loss: 1.575272798538208\n",
            "Saved best model at epoch 28\n",
            "Epoch: 29, Batch: 0, Average Loss: 1.5904474258422852\n",
            "Epoch: 29, Batch: 25, Average Loss: 1.6171694076978242\n",
            "Epoch: 29, Batch: 50, Average Loss: 1.6162864460664637\n",
            "Epoch: 29, Batch: 75, Average Loss: 1.6140112720037763\n",
            "Epoch: 29, Batch: 100, Average Loss: 1.6153243225399812\n",
            "Epoch: 29, Validation Loss: 1.572395920753479\n",
            "Saved best model at epoch 29\n",
            "Epoch: 30, Batch: 0, Average Loss: 1.5879218578338623\n",
            "Epoch: 30, Batch: 25, Average Loss: 1.6145330529946547\n",
            "Epoch: 30, Batch: 50, Average Loss: 1.6136681566051407\n",
            "Epoch: 30, Batch: 75, Average Loss: 1.6113837675044411\n",
            "Epoch: 30, Batch: 100, Average Loss: 1.612698773346325\n",
            "Epoch: 30, Validation Loss: 1.5695152282714844\n",
            "Saved best model at epoch 30\n",
            "Epoch: 31, Batch: 0, Average Loss: 1.5853300094604492\n",
            "Epoch: 31, Batch: 25, Average Loss: 1.6118232286893404\n",
            "Epoch: 31, Batch: 50, Average Loss: 1.6109797113081987\n",
            "Epoch: 31, Batch: 75, Average Loss: 1.6086860863785994\n",
            "Epoch: 31, Batch: 100, Average Loss: 1.6100027466764544\n",
            "Epoch: 31, Validation Loss: 1.5665935277938843\n",
            "Saved best model at epoch 31\n",
            "Epoch: 32, Batch: 0, Average Loss: 1.5826798677444458\n",
            "Epoch: 32, Batch: 25, Average Loss: 1.6090672107843251\n",
            "Epoch: 32, Batch: 50, Average Loss: 1.6082441806793213\n",
            "Epoch: 32, Batch: 75, Average Loss: 1.6059409128992181\n",
            "Epoch: 32, Batch: 100, Average Loss: 1.6072593360844225\n",
            "Epoch: 32, Validation Loss: 1.563670039176941\n",
            "Saved best model at epoch 32\n",
            "Epoch: 33, Batch: 0, Average Loss: 1.5799829959869385\n",
            "Epoch: 33, Batch: 25, Average Loss: 1.6062498230200548\n",
            "Epoch: 33, Batch: 50, Average Loss: 1.6054487906250299\n",
            "Epoch: 33, Batch: 75, Average Loss: 1.603136547301945\n",
            "Epoch: 33, Batch: 100, Average Loss: 1.6044565238574944\n",
            "Epoch: 33, Validation Loss: 1.5607153177261353\n",
            "Saved best model at epoch 33\n",
            "Epoch: 34, Batch: 0, Average Loss: 1.5772298574447632\n",
            "Epoch: 34, Batch: 25, Average Loss: 1.6034001891429608\n",
            "Epoch: 34, Batch: 50, Average Loss: 1.6026222191604913\n",
            "Epoch: 34, Batch: 75, Average Loss: 1.6003006837869946\n",
            "Epoch: 34, Batch: 100, Average Loss: 1.6016221636592751\n",
            "Epoch: 34, Validation Loss: 1.5577257871627808\n",
            "Saved best model at epoch 34\n",
            "Epoch: 35, Batch: 0, Average Loss: 1.5744394063949585\n",
            "Epoch: 35, Batch: 25, Average Loss: 1.6005140634683461\n",
            "Epoch: 35, Batch: 50, Average Loss: 1.5997580196343215\n",
            "Epoch: 35, Batch: 75, Average Loss: 1.597427259934576\n",
            "Epoch: 35, Batch: 100, Average Loss: 1.5987502631574575\n",
            "Epoch: 35, Validation Loss: 1.5547640323638916\n",
            "Saved best model at epoch 35\n",
            "Epoch: 36, Batch: 0, Average Loss: 1.5716410875320435\n",
            "Epoch: 36, Batch: 25, Average Loss: 1.5976046598874605\n",
            "Epoch: 36, Batch: 50, Average Loss: 1.5968708921881283\n",
            "Epoch: 36, Batch: 75, Average Loss: 1.5945305698796322\n",
            "Epoch: 36, Batch: 100, Average Loss: 1.5958549480627078\n",
            "Epoch: 36, Validation Loss: 1.5517741441726685\n",
            "Saved best model at epoch 36\n",
            "Epoch: 37, Batch: 0, Average Loss: 1.5688064098358154\n",
            "Epoch: 37, Batch: 25, Average Loss: 1.5946609469560475\n",
            "Epoch: 37, Batch: 50, Average Loss: 1.5939505053501504\n",
            "Epoch: 37, Batch: 75, Average Loss: 1.5916008353233337\n",
            "Epoch: 37, Batch: 100, Average Loss: 1.5929267382857824\n",
            "Epoch: 37, Validation Loss: 1.5487629175186157\n",
            "Saved best model at epoch 37\n",
            "Epoch: 38, Batch: 0, Average Loss: 1.565940499305725\n",
            "Epoch: 38, Batch: 25, Average Loss: 1.5916852263303904\n",
            "Epoch: 38, Batch: 50, Average Loss: 1.590998233533373\n",
            "Epoch: 38, Batch: 75, Average Loss: 1.588639287572158\n",
            "Epoch: 38, Batch: 100, Average Loss: 1.589966354983868\n",
            "Epoch: 38, Validation Loss: 1.5457619428634644\n",
            "Saved best model at epoch 38\n",
            "Epoch: 39, Batch: 0, Average Loss: 1.5630462169647217\n",
            "Epoch: 39, Batch: 25, Average Loss: 1.5886814961066613\n",
            "Epoch: 39, Batch: 50, Average Loss: 1.5880182350383085\n",
            "Epoch: 39, Batch: 75, Average Loss: 1.5856497475975437\n",
            "Epoch: 39, Batch: 100, Average Loss: 1.586978267915178\n",
            "Epoch: 39, Validation Loss: 1.5427393913269043\n",
            "Saved best model at epoch 39\n",
            "Epoch: 40, Batch: 0, Average Loss: 1.5601304769515991\n",
            "Epoch: 40, Batch: 25, Average Loss: 1.585666142977201\n",
            "Epoch: 40, Batch: 50, Average Loss: 1.585027486670251\n",
            "Epoch: 40, Batch: 75, Average Loss: 1.5826494834925\n",
            "Epoch: 40, Batch: 100, Average Loss: 1.5839792643443193\n",
            "Epoch: 40, Validation Loss: 1.5397109985351562\n",
            "Saved best model at epoch 40\n",
            "Epoch: 41, Batch: 0, Average Loss: 1.5571997165679932\n",
            "Epoch: 41, Batch: 25, Average Loss: 1.5826252240401049\n",
            "Epoch: 41, Batch: 50, Average Loss: 1.582010311238906\n",
            "Epoch: 41, Batch: 75, Average Loss: 1.5796229478559995\n",
            "Epoch: 41, Batch: 100, Average Loss: 1.5809537845082802\n",
            "Epoch: 41, Validation Loss: 1.5366815328598022\n",
            "Saved best model at epoch 41\n",
            "Epoch: 42, Batch: 0, Average Loss: 1.5542491674423218\n",
            "Epoch: 42, Batch: 25, Average Loss: 1.5795688308202303\n",
            "Epoch: 42, Batch: 50, Average Loss: 1.5789782369838041\n",
            "Epoch: 42, Batch: 75, Average Loss: 1.5765813510668905\n",
            "Epoch: 42, Batch: 100, Average Loss: 1.577913244171898\n",
            "Epoch: 42, Validation Loss: 1.5336401462554932\n",
            "Saved best model at epoch 42\n",
            "Epoch: 43, Batch: 0, Average Loss: 1.5512845516204834\n",
            "Epoch: 43, Batch: 25, Average Loss: 1.576498072880965\n",
            "Epoch: 43, Batch: 50, Average Loss: 1.575932173167958\n",
            "Epoch: 43, Batch: 75, Average Loss: 1.5735258099279905\n",
            "Epoch: 43, Batch: 100, Average Loss: 1.5748587197596484\n",
            "Epoch: 43, Validation Loss: 1.530602216720581\n",
            "Saved best model at epoch 43\n",
            "Epoch: 44, Batch: 0, Average Loss: 1.5483089685440063\n",
            "Epoch: 44, Batch: 25, Average Loss: 1.5734163018373342\n",
            "Epoch: 44, Batch: 50, Average Loss: 1.5728750649620504\n",
            "Epoch: 44, Batch: 75, Average Loss: 1.570459202716225\n",
            "Epoch: 44, Batch: 100, Average Loss: 1.5717930746550608\n",
            "Epoch: 44, Validation Loss: 1.5275559425354004\n",
            "Saved best model at epoch 44\n",
            "Epoch: 45, Batch: 0, Average Loss: 1.545323133468628\n",
            "Epoch: 45, Batch: 25, Average Loss: 1.5703237423529992\n",
            "Epoch: 45, Batch: 50, Average Loss: 1.5698074312771069\n",
            "Epoch: 45, Batch: 75, Average Loss: 1.5673823544853611\n",
            "Epoch: 45, Batch: 100, Average Loss: 1.5687173333498512\n",
            "Epoch: 45, Validation Loss: 1.5244886875152588\n",
            "Saved best model at epoch 45\n",
            "Epoch: 46, Batch: 0, Average Loss: 1.5423238277435303\n",
            "Epoch: 46, Batch: 25, Average Loss: 1.567216002024137\n",
            "Epoch: 46, Batch: 50, Average Loss: 1.5667251254998\n",
            "Epoch: 46, Batch: 75, Average Loss: 1.5642905611740916\n",
            "Epoch: 46, Batch: 100, Average Loss: 1.5656264076138486\n",
            "Epoch: 46, Validation Loss: 1.5214462280273438\n",
            "Saved best model at epoch 46\n",
            "Epoch: 47, Batch: 0, Average Loss: 1.539320707321167\n",
            "Epoch: 47, Batch: 25, Average Loss: 1.5641081929206848\n",
            "Epoch: 47, Batch: 50, Average Loss: 1.5636418707230513\n",
            "Epoch: 47, Batch: 75, Average Loss: 1.5611976134149652\n",
            "Epoch: 47, Batch: 100, Average Loss: 1.5625341906405912\n",
            "Epoch: 47, Validation Loss: 1.5183862447738647\n",
            "Saved best model at epoch 47\n",
            "Epoch: 48, Batch: 0, Average Loss: 1.5363109111785889\n",
            "Epoch: 48, Batch: 25, Average Loss: 1.560991420195653\n",
            "Epoch: 48, Batch: 50, Average Loss: 1.5605502105226703\n",
            "Epoch: 48, Batch: 75, Average Loss: 1.5580963994327344\n",
            "Epoch: 48, Batch: 100, Average Loss: 1.5594337423249047\n",
            "Epoch: 48, Validation Loss: 1.5153272151947021\n",
            "Saved best model at epoch 48\n",
            "Epoch: 49, Batch: 0, Average Loss: 1.5332961082458496\n",
            "Epoch: 49, Batch: 25, Average Loss: 1.5578697461348314\n",
            "Epoch: 49, Batch: 50, Average Loss: 1.5574534523720835\n",
            "Epoch: 49, Batch: 75, Average Loss: 1.5549901708176261\n",
            "Epoch: 49, Batch: 100, Average Loss: 1.556328278957027\n",
            "Epoch: 49, Validation Loss: 1.5122638940811157\n",
            "Saved best model at epoch 49\n",
            "Epoch: 50, Batch: 0, Average Loss: 1.5302765369415283\n",
            "Epoch: 50, Batch: 25, Average Loss: 1.5547428681300237\n",
            "Epoch: 50, Batch: 50, Average Loss: 1.5543516850938983\n",
            "Epoch: 50, Batch: 75, Average Loss: 1.5518790075653477\n",
            "Epoch: 50, Batch: 100, Average Loss: 1.5532178748952281\n",
            "Epoch: 50, Validation Loss: 1.5091910362243652\n",
            "Saved best model at epoch 50\n",
            "Epoch: 51, Batch: 0, Average Loss: 1.527251958847046\n",
            "Epoch: 51, Batch: 25, Average Loss: 1.5516108870506287\n",
            "Epoch: 51, Batch: 50, Average Loss: 1.551245044259464\n",
            "Epoch: 51, Batch: 75, Average Loss: 1.548762875167947\n",
            "Epoch: 51, Batch: 100, Average Loss: 1.550102415651378\n",
            "Epoch: 51, Validation Loss: 1.5061277151107788\n",
            "Saved best model at epoch 51\n",
            "Epoch: 52, Batch: 0, Average Loss: 1.5242253541946411\n",
            "Epoch: 52, Batch: 25, Average Loss: 1.548476150402656\n",
            "Epoch: 52, Batch: 50, Average Loss: 1.5481354372174132\n",
            "Epoch: 52, Batch: 75, Average Loss: 1.5456437405787016\n",
            "Epoch: 52, Batch: 100, Average Loss: 1.5469839136199195\n",
            "Epoch: 52, Validation Loss: 1.5030561685562134\n",
            "Saved best model at epoch 52\n",
            "Epoch: 53, Batch: 0, Average Loss: 1.5211951732635498\n",
            "Epoch: 53, Batch: 25, Average Loss: 1.5453376311522264\n",
            "Epoch: 53, Batch: 50, Average Loss: 1.5450221463745715\n",
            "Epoch: 53, Batch: 75, Average Loss: 1.5425210532389189\n",
            "Epoch: 53, Batch: 100, Average Loss: 1.5438618789805043\n",
            "Epoch: 53, Validation Loss: 1.4999818801879883\n",
            "Saved best model at epoch 53\n",
            "Epoch: 54, Batch: 0, Average Loss: 1.518162727355957\n",
            "Epoch: 54, Batch: 25, Average Loss: 1.5421965076373174\n",
            "Epoch: 54, Batch: 50, Average Loss: 1.5419063194125306\n",
            "Epoch: 54, Batch: 75, Average Loss: 1.53939572604079\n",
            "Epoch: 54, Batch: 100, Average Loss: 1.5407371284938094\n",
            "Epoch: 54, Validation Loss: 1.496904730796814\n",
            "Saved best model at epoch 54\n",
            "Epoch: 55, Batch: 0, Average Loss: 1.5151280164718628\n",
            "Epoch: 55, Batch: 25, Average Loss: 1.5390527890278742\n",
            "Epoch: 55, Batch: 50, Average Loss: 1.538787890883053\n",
            "Epoch: 55, Batch: 75, Average Loss: 1.5362678374114789\n",
            "Epoch: 55, Batch: 100, Average Loss: 1.5376098167778243\n",
            "Epoch: 55, Validation Loss: 1.493825078010559\n",
            "Saved best model at epoch 55\n",
            "Epoch: 56, Batch: 0, Average Loss: 1.5120913982391357\n",
            "Epoch: 56, Batch: 25, Average Loss: 1.5359041369878328\n",
            "Epoch: 56, Batch: 50, Average Loss: 1.5356659468482523\n",
            "Epoch: 56, Batch: 75, Average Loss: 1.5331371991257918\n",
            "Epoch: 56, Batch: 100, Average Loss: 1.5344798765560188\n",
            "Epoch: 56, Validation Loss: 1.4907230138778687\n",
            "Saved best model at epoch 56\n",
            "Epoch: 57, Batch: 0, Average Loss: 1.509048581123352\n",
            "Epoch: 57, Batch: 25, Average Loss: 1.532753981076754\n",
            "Epoch: 57, Batch: 50, Average Loss: 1.5325405550938027\n",
            "Epoch: 57, Batch: 75, Average Loss: 1.5300018803069466\n",
            "Epoch: 57, Batch: 100, Average Loss: 1.5313449437075322\n",
            "Epoch: 57, Validation Loss: 1.4876528978347778\n",
            "Saved best model at epoch 57\n",
            "Epoch: 58, Batch: 0, Average Loss: 1.5060102939605713\n",
            "Epoch: 58, Batch: 25, Average Loss: 1.5296045358364398\n",
            "Epoch: 58, Batch: 50, Average Loss: 1.5294160445531209\n",
            "Epoch: 58, Batch: 75, Average Loss: 1.5268677300528477\n",
            "Epoch: 58, Batch: 100, Average Loss: 1.5282110353507619\n",
            "Epoch: 58, Validation Loss: 1.4845638275146484\n",
            "Saved best model at epoch 58\n",
            "Epoch: 59, Batch: 0, Average Loss: 1.5029737949371338\n",
            "Epoch: 59, Batch: 25, Average Loss: 1.5264491576414843\n",
            "Epoch: 59, Batch: 50, Average Loss: 1.5262852977303898\n",
            "Epoch: 59, Batch: 75, Average Loss: 1.523727242883883\n",
            "Epoch: 59, Batch: 100, Average Loss: 1.525071092171244\n",
            "Epoch: 59, Validation Loss: 1.4814742803573608\n",
            "Saved best model at epoch 59\n",
            "Epoch: 60, Batch: 0, Average Loss: 1.499924898147583\n",
            "Epoch: 60, Batch: 25, Average Loss: 1.5232922755754912\n",
            "Epoch: 60, Batch: 50, Average Loss: 1.5231539899227666\n",
            "Epoch: 60, Batch: 75, Average Loss: 1.520586537687402\n",
            "Epoch: 60, Batch: 100, Average Loss: 1.5219309542438773\n",
            "Epoch: 60, Validation Loss: 1.4783775806427002\n",
            "Saved best model at epoch 60\n",
            "Epoch: 61, Batch: 0, Average Loss: 1.4968786239624023\n",
            "Epoch: 61, Batch: 25, Average Loss: 1.5201354576991155\n",
            "Epoch: 61, Batch: 50, Average Loss: 1.5200225418689204\n",
            "Epoch: 61, Batch: 75, Average Loss: 1.5174459736598165\n",
            "Epoch: 61, Batch: 100, Average Loss: 1.5187910464730594\n",
            "Epoch: 61, Validation Loss: 1.4752986431121826\n",
            "Saved best model at epoch 61\n",
            "Epoch: 62, Batch: 0, Average Loss: 1.4938380718231201\n",
            "Epoch: 62, Batch: 25, Average Loss: 1.5169819730978746\n",
            "Epoch: 62, Batch: 50, Average Loss: 1.5168940413231944\n",
            "Epoch: 62, Batch: 75, Average Loss: 1.5143079977286489\n",
            "Epoch: 62, Batch: 100, Average Loss: 1.5156535972463023\n",
            "Epoch: 62, Validation Loss: 1.472185492515564\n",
            "Saved best model at epoch 62\n",
            "Epoch: 63, Batch: 0, Average Loss: 1.490785837173462\n",
            "Epoch: 63, Batch: 25, Average Loss: 1.5138185345209563\n",
            "Epoch: 63, Batch: 50, Average Loss: 1.513756880573198\n",
            "Epoch: 63, Batch: 75, Average Loss: 1.5111615202928845\n",
            "Epoch: 63, Batch: 100, Average Loss: 1.5125075673112776\n",
            "Epoch: 63, Validation Loss: 1.4690840244293213\n",
            "Saved best model at epoch 63\n",
            "Epoch: 64, Batch: 0, Average Loss: 1.4877382516860962\n",
            "Epoch: 64, Batch: 25, Average Loss: 1.510657521394583\n",
            "Epoch: 64, Batch: 50, Average Loss: 1.5106215243246042\n",
            "Epoch: 64, Batch: 75, Average Loss: 1.5080168560931557\n",
            "Epoch: 64, Batch: 100, Average Loss: 1.5093635214437353\n",
            "Epoch: 64, Validation Loss: 1.4659924507141113\n",
            "Saved best model at epoch 64\n",
            "Epoch: 65, Batch: 0, Average Loss: 1.4846891164779663\n",
            "Epoch: 65, Batch: 25, Average Loss: 1.5074949952272267\n",
            "Epoch: 65, Batch: 50, Average Loss: 1.5074838353138345\n",
            "Epoch: 65, Batch: 75, Average Loss: 1.5048693230277614\n",
            "Epoch: 65, Batch: 100, Average Loss: 1.506216307677845\n",
            "Epoch: 65, Validation Loss: 1.462897777557373\n",
            "Saved best model at epoch 65\n",
            "Epoch: 66, Batch: 0, Average Loss: 1.4816389083862305\n",
            "Epoch: 66, Batch: 25, Average Loss: 1.5043324415500348\n",
            "Epoch: 66, Batch: 50, Average Loss: 1.5043466652140898\n",
            "Epoch: 66, Batch: 75, Average Loss: 1.5017235279083252\n",
            "Epoch: 66, Batch: 100, Average Loss: 1.5030710059817474\n",
            "Epoch: 66, Validation Loss: 1.4598021507263184\n",
            "Saved best model at epoch 66\n",
            "Epoch: 67, Batch: 0, Average Loss: 1.4785770177841187\n",
            "Epoch: 67, Batch: 25, Average Loss: 1.5011589114482586\n",
            "Epoch: 67, Batch: 50, Average Loss: 1.5011984530617208\n",
            "Epoch: 67, Batch: 75, Average Loss: 1.4985653271800594\n",
            "Epoch: 67, Batch: 100, Average Loss: 1.4999131566227073\n",
            "Epoch: 67, Validation Loss: 1.4567009210586548\n",
            "Saved best model at epoch 67\n",
            "Epoch: 68, Batch: 0, Average Loss: 1.4755315780639648\n",
            "Epoch: 68, Batch: 25, Average Loss: 1.4979963073363671\n",
            "Epoch: 68, Batch: 50, Average Loss: 1.498061084279827\n",
            "Epoch: 68, Batch: 75, Average Loss: 1.4954182709518231\n",
            "Epoch: 68, Batch: 100, Average Loss: 1.4967664810690549\n",
            "Epoch: 68, Validation Loss: 1.453598976135254\n",
            "Saved best model at epoch 68\n",
            "Epoch: 69, Batch: 0, Average Loss: 1.4724832773208618\n",
            "Epoch: 69, Batch: 25, Average Loss: 1.4948335152405958\n",
            "Epoch: 69, Batch: 50, Average Loss: 1.4949236780989403\n",
            "Epoch: 69, Batch: 75, Average Loss: 1.4922712868765782\n",
            "Epoch: 69, Batch: 100, Average Loss: 1.4936199164626622\n",
            "Epoch: 69, Validation Loss: 1.450497031211853\n",
            "Saved best model at epoch 69\n",
            "Epoch: 70, Batch: 0, Average Loss: 1.4694349765777588\n",
            "Epoch: 70, Batch: 25, Average Loss: 1.4916709569784312\n",
            "Epoch: 70, Batch: 50, Average Loss: 1.4917865243612551\n",
            "Epoch: 70, Batch: 75, Average Loss: 1.4891245663166046\n",
            "Epoch: 70, Batch: 100, Average Loss: 1.4904736245032584\n",
            "Epoch: 70, Validation Loss: 1.4473943710327148\n",
            "Saved best model at epoch 70\n",
            "Epoch: 71, Batch: 0, Average Loss: 1.466387391090393\n",
            "Epoch: 71, Batch: 25, Average Loss: 1.4885084674908564\n",
            "Epoch: 71, Batch: 50, Average Loss: 1.488649478145674\n",
            "Epoch: 71, Batch: 75, Average Loss: 1.485977966534464\n",
            "Epoch: 71, Batch: 100, Average Loss: 1.486987841011274\n",
            "Epoch: 71, Validation Loss: 1.4441206455230713\n",
            "Saved best model at epoch 71\n",
            "Epoch: 72, Batch: 0, Average Loss: 1.4618425369262695\n",
            "Epoch: 72, Batch: 25, Average Loss: 1.4853217968573937\n",
            "Epoch: 72, Batch: 50, Average Loss: 1.4855339410258275\n",
            "Epoch: 72, Batch: 75, Average Loss: 1.4828504716095172\n",
            "Epoch: 72, Batch: 100, Average Loss: 1.4838556629596371\n",
            "Epoch: 72, Validation Loss: 1.4414117336273193\n",
            "Saved best model at epoch 72\n",
            "Epoch: 73, Batch: 0, Average Loss: 1.459162950515747\n",
            "Epoch: 73, Batch: 25, Average Loss: 1.4819390865472646\n",
            "Epoch: 73, Batch: 50, Average Loss: 1.4822978248783187\n",
            "Epoch: 73, Batch: 75, Average Loss: 1.4794753789901733\n",
            "Epoch: 73, Batch: 100, Average Loss: 1.4806265984431353\n",
            "Epoch: 73, Validation Loss: 1.4381992816925049\n",
            "Saved best model at epoch 73\n",
            "Epoch: 74, Batch: 0, Average Loss: 1.4564733505249023\n",
            "Epoch: 74, Batch: 25, Average Loss: 1.4783942103385925\n",
            "Epoch: 74, Batch: 50, Average Loss: 1.478986197826909\n",
            "Epoch: 74, Batch: 75, Average Loss: 1.4762390946087085\n",
            "Epoch: 74, Batch: 100, Average Loss: 1.4775018208097703\n",
            "Epoch: 74, Validation Loss: 1.4351327419281006\n",
            "Saved best model at epoch 74\n",
            "Epoch: 75, Batch: 0, Average Loss: 1.453751802444458\n",
            "Epoch: 75, Batch: 25, Average Loss: 1.4755300191732554\n",
            "Epoch: 75, Batch: 50, Average Loss: 1.4759630689433976\n",
            "Epoch: 75, Batch: 75, Average Loss: 1.4732249190932827\n",
            "Epoch: 75, Batch: 100, Average Loss: 1.474536892211083\n",
            "Epoch: 75, Validation Loss: 1.432074785232544\n",
            "Saved best model at epoch 75\n",
            "Epoch: 76, Batch: 0, Average Loss: 1.4510139226913452\n",
            "Epoch: 76, Batch: 25, Average Loss: 1.4726824989685645\n",
            "Epoch: 76, Batch: 50, Average Loss: 1.4729825118008781\n",
            "Epoch: 76, Batch: 75, Average Loss: 1.4702541530132294\n",
            "Epoch: 76, Batch: 100, Average Loss: 1.4716005018441984\n",
            "Epoch: 76, Validation Loss: 1.4289830923080444\n",
            "Saved best model at epoch 76\n",
            "Epoch: 77, Batch: 0, Average Loss: 1.4482696056365967\n",
            "Epoch: 77, Batch: 25, Average Loss: 1.4699484293277447\n",
            "Epoch: 77, Batch: 50, Average Loss: 1.470239283991795\n",
            "Epoch: 77, Batch: 75, Average Loss: 1.4675114123444808\n",
            "Epoch: 77, Batch: 100, Average Loss: 1.4688641954176496\n",
            "Epoch: 77, Validation Loss: 1.4258416891098022\n",
            "Saved best model at epoch 77\n",
            "Epoch: 78, Batch: 0, Average Loss: 1.4457728862762451\n",
            "Epoch: 78, Batch: 25, Average Loss: 1.4673323310338533\n",
            "Epoch: 78, Batch: 50, Average Loss: 1.4676407926222856\n",
            "Epoch: 78, Batch: 75, Average Loss: 1.4649039177518142\n",
            "Epoch: 78, Batch: 100, Average Loss: 1.4662588763945172\n",
            "Epoch: 78, Validation Loss: 1.4226963520050049\n",
            "Saved best model at epoch 78\n",
            "Epoch: 79, Batch: 0, Average Loss: 1.443237066268921\n",
            "Epoch: 79, Batch: 25, Average Loss: 1.4646797226025507\n",
            "Epoch: 79, Batch: 50, Average Loss: 1.4650055497300392\n",
            "Epoch: 79, Batch: 75, Average Loss: 1.4622592612316734\n",
            "Epoch: 79, Batch: 100, Average Loss: 1.463616158702586\n",
            "Epoch: 79, Validation Loss: 1.4195339679718018\n",
            "Saved best model at epoch 79\n",
            "Epoch: 80, Batch: 0, Average Loss: 1.4406658411026\n",
            "Epoch: 80, Batch: 25, Average Loss: 1.4619908607923067\n",
            "Epoch: 80, Batch: 50, Average Loss: 1.462334343031341\n",
            "Epoch: 80, Batch: 75, Average Loss: 1.4595785878206555\n",
            "Epoch: 80, Batch: 100, Average Loss: 1.4609373855118704\n",
            "Epoch: 80, Validation Loss: 1.416351318359375\n",
            "Saved best model at epoch 80\n",
            "Epoch: 81, Batch: 0, Average Loss: 1.4380604028701782\n",
            "Epoch: 81, Batch: 25, Average Loss: 1.459267033980443\n",
            "Epoch: 81, Batch: 50, Average Loss: 1.4596283015082865\n",
            "Epoch: 81, Batch: 75, Average Loss: 1.4568630347126408\n",
            "Epoch: 81, Batch: 100, Average Loss: 1.458223825634116\n",
            "Epoch: 81, Validation Loss: 1.4131364822387695\n",
            "Saved best model at epoch 81\n",
            "Epoch: 82, Batch: 0, Average Loss: 1.4354164600372314\n",
            "Epoch: 82, Batch: 25, Average Loss: 1.4565020295289846\n",
            "Epoch: 82, Batch: 50, Average Loss: 1.456881838686326\n",
            "Epoch: 82, Batch: 75, Average Loss: 1.4541070053451939\n",
            "Epoch: 82, Batch: 100, Average Loss: 1.4554696236506548\n",
            "Epoch: 82, Validation Loss: 1.4099124670028687\n",
            "Saved best model at epoch 82\n",
            "Epoch: 83, Batch: 0, Average Loss: 1.432739019393921\n",
            "Epoch: 83, Batch: 25, Average Loss: 1.453703055014977\n",
            "Epoch: 83, Batch: 50, Average Loss: 1.4541019645391726\n",
            "Epoch: 83, Batch: 75, Average Loss: 1.4513178279525356\n",
            "Epoch: 83, Batch: 100, Average Loss: 1.452682377088188\n",
            "Epoch: 83, Validation Loss: 1.4066522121429443\n",
            "Saved best model at epoch 83\n",
            "Epoch: 84, Batch: 0, Average Loss: 1.4300228357315063\n",
            "Epoch: 84, Batch: 25, Average Loss: 1.4508635585124676\n",
            "Epoch: 84, Batch: 50, Average Loss: 1.4512807060690487\n",
            "Epoch: 84, Batch: 75, Average Loss: 1.448486310871024\n",
            "Epoch: 84, Batch: 100, Average Loss: 1.4498523072441025\n",
            "Epoch: 84, Validation Loss: 1.4033812284469604\n",
            "Saved best model at epoch 84\n",
            "Epoch: 85, Batch: 0, Average Loss: 1.4272727966308594\n",
            "Epoch: 85, Batch: 25, Average Loss: 1.4479934527323797\n",
            "Epoch: 85, Batch: 50, Average Loss: 1.4484295237298106\n",
            "Epoch: 85, Batch: 75, Average Loss: 1.4456251169505872\n",
            "Epoch: 85, Batch: 100, Average Loss: 1.4469927490347683\n",
            "Epoch: 85, Validation Loss: 1.400099515914917\n",
            "Saved best model at epoch 85\n",
            "Epoch: 86, Batch: 0, Average Loss: 1.4244935512542725\n",
            "Epoch: 86, Batch: 25, Average Loss: 1.4450906560971186\n",
            "Epoch: 86, Batch: 50, Average Loss: 1.4455464797861435\n",
            "Epoch: 86, Batch: 75, Average Loss: 1.4427322541412555\n",
            "Epoch: 86, Batch: 100, Average Loss: 1.4441014858755734\n",
            "Epoch: 86, Validation Loss: 1.3967781066894531\n",
            "Saved best model at epoch 86\n",
            "Epoch: 87, Batch: 0, Average Loss: 1.4216830730438232\n",
            "Epoch: 87, Batch: 25, Average Loss: 1.4421610144468455\n",
            "Epoch: 87, Batch: 50, Average Loss: 1.4426361322402954\n",
            "Epoch: 87, Batch: 75, Average Loss: 1.4398116830148195\n",
            "Epoch: 87, Batch: 100, Average Loss: 1.4411824906226431\n",
            "Epoch: 87, Validation Loss: 1.3934454917907715\n",
            "Saved best model at epoch 87\n",
            "Epoch: 88, Batch: 0, Average Loss: 1.4188435077667236\n",
            "Epoch: 88, Batch: 25, Average Loss: 1.4391930745198176\n",
            "Epoch: 88, Batch: 50, Average Loss: 1.4396881145589493\n",
            "Epoch: 88, Batch: 75, Average Loss: 1.4368535060631602\n",
            "Epoch: 88, Batch: 100, Average Loss: 1.4382258606429148\n",
            "Epoch: 88, Validation Loss: 1.3900896310806274\n",
            "Saved best model at epoch 88\n",
            "Epoch: 89, Batch: 0, Average Loss: 1.4159646034240723\n",
            "Epoch: 89, Batch: 25, Average Loss: 1.4361847318135774\n",
            "Epoch: 89, Batch: 50, Average Loss: 1.4367002295512779\n",
            "Epoch: 89, Batch: 75, Average Loss: 1.4338554677210356\n",
            "Epoch: 89, Batch: 100, Average Loss: 1.4352293640080065\n",
            "Epoch: 89, Validation Loss: 1.3867107629776\n",
            "Saved best model at epoch 89\n",
            "Epoch: 90, Batch: 0, Average Loss: 1.413050651550293\n",
            "Epoch: 90, Batch: 25, Average Loss: 1.4331410389680128\n",
            "Epoch: 90, Batch: 50, Average Loss: 1.4336780940785128\n",
            "Epoch: 90, Batch: 75, Average Loss: 1.4308234955135144\n",
            "Epoch: 90, Batch: 100, Average Loss: 1.4321990980960355\n",
            "Epoch: 90, Validation Loss: 1.383299469947815\n",
            "Saved best model at epoch 90\n",
            "Epoch: 91, Batch: 0, Average Loss: 1.410101056098938\n",
            "Epoch: 91, Batch: 25, Average Loss: 1.430062202306894\n",
            "Epoch: 91, Batch: 50, Average Loss: 1.4306202636045569\n",
            "Epoch: 91, Batch: 75, Average Loss: 1.4277552758392535\n",
            "Epoch: 91, Batch: 100, Average Loss: 1.4291323140116021\n",
            "Epoch: 91, Validation Loss: 1.3798863887786865\n",
            "Saved best model at epoch 91\n",
            "Epoch: 92, Batch: 0, Average Loss: 1.4071232080459595\n",
            "Epoch: 92, Batch: 25, Average Loss: 1.42695591541437\n",
            "Epoch: 92, Batch: 50, Average Loss: 1.4275353352228801\n",
            "Epoch: 92, Batch: 75, Average Loss: 1.4246599940877211\n",
            "Epoch: 92, Batch: 100, Average Loss: 1.4260384965651105\n",
            "Epoch: 92, Validation Loss: 1.3764588832855225\n",
            "Saved best model at epoch 92\n",
            "Epoch: 93, Batch: 0, Average Loss: 1.4041153192520142\n",
            "Epoch: 93, Batch: 25, Average Loss: 1.423817712527055\n",
            "Epoch: 93, Batch: 50, Average Loss: 1.4244187298943014\n",
            "Epoch: 93, Batch: 75, Average Loss: 1.4215329054154848\n",
            "Epoch: 93, Batch: 100, Average Loss: 1.422912844336859\n",
            "Epoch: 93, Validation Loss: 1.3729923963546753\n",
            "Saved best model at epoch 93\n",
            "Epoch: 94, Batch: 0, Average Loss: 1.401073694229126\n",
            "Epoch: 94, Batch: 25, Average Loss: 1.4206439302517817\n",
            "Epoch: 94, Batch: 50, Average Loss: 1.4212678016400804\n",
            "Epoch: 94, Batch: 75, Average Loss: 1.4183717699427354\n",
            "Epoch: 94, Batch: 100, Average Loss: 1.4197532233625356\n",
            "Epoch: 94, Validation Loss: 1.3695333003997803\n",
            "Saved best model at epoch 94\n",
            "Epoch: 95, Batch: 0, Average Loss: 1.3980094194412231\n",
            "Epoch: 95, Batch: 25, Average Loss: 1.4174511157549345\n",
            "Epoch: 95, Batch: 50, Average Loss: 1.4180973394244325\n",
            "Epoch: 95, Batch: 75, Average Loss: 1.415190721813001\n",
            "Epoch: 95, Batch: 100, Average Loss: 1.4165735374582875\n",
            "Epoch: 95, Validation Loss: 1.3660610914230347\n",
            "Saved best model at epoch 95\n",
            "Epoch: 96, Batch: 0, Average Loss: 1.3949216604232788\n",
            "Epoch: 96, Batch: 25, Average Loss: 1.4142327308654785\n",
            "Epoch: 96, Batch: 50, Average Loss: 1.4149015019921696\n",
            "Epoch: 96, Batch: 75, Average Loss: 1.4119843809228194\n",
            "Epoch: 96, Batch: 100, Average Loss: 1.4133686252159647\n",
            "Epoch: 96, Validation Loss: 1.3625646829605103\n",
            "Saved best model at epoch 96\n",
            "Epoch: 97, Batch: 0, Average Loss: 1.3918085098266602\n",
            "Epoch: 97, Batch: 25, Average Loss: 1.4109878035692067\n",
            "Epoch: 97, Batch: 50, Average Loss: 1.4116795273388134\n",
            "Epoch: 97, Batch: 75, Average Loss: 1.4087516226266559\n",
            "Epoch: 97, Batch: 100, Average Loss: 1.410137157629032\n",
            "Epoch: 97, Validation Loss: 1.3590530157089233\n",
            "Saved best model at epoch 97\n",
            "Epoch: 98, Batch: 0, Average Loss: 1.3886711597442627\n",
            "Epoch: 98, Batch: 25, Average Loss: 1.4077186722021837\n",
            "Epoch: 98, Batch: 50, Average Loss: 1.4084337739383472\n",
            "Epoch: 98, Batch: 75, Average Loss: 1.405495235794469\n",
            "Epoch: 98, Batch: 100, Average Loss: 1.4068821408961079\n",
            "Epoch: 98, Validation Loss: 1.3555291891098022\n",
            "Saved best model at epoch 98\n",
            "Epoch: 99, Batch: 0, Average Loss: 1.3855127096176147\n",
            "Epoch: 99, Batch: 25, Average Loss: 1.4044284086961012\n",
            "Epoch: 99, Batch: 50, Average Loss: 1.4051671729368322\n",
            "Epoch: 99, Batch: 75, Average Loss: 1.4022179685140912\n",
            "Epoch: 99, Batch: 100, Average Loss: 1.4036062235879425\n",
            "Epoch: 99, Validation Loss: 1.351991891860962\n",
            "Saved best model at epoch 99\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 100\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    # Train\n",
        "    train_loss = model.train(train_batch, epoch)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss = model.compute_loss((\n",
        "        torch.FloatTensor(val_batch[0]).to(device),  # state\n",
        "        torch.FloatTensor(val_batch[1]).to(device),  # next_state\n",
        "        torch.LongTensor(val_batch[2]).to(device),   # action\n",
        "        torch.LongTensor(val_batch[3]).to(device),   # next_action\n",
        "        torch.FloatTensor(val_batch[4]).to(device),  # reward\n",
        "        torch.FloatTensor(val_batch[5]).to(device),  # done\n",
        "        torch.FloatTensor(val_batch[6]).to(device)   # SOFA\n",
        "    )).item()\n",
        "    print(f'Epoch: {epoch}, Validation Loss: {val_loss}')\n",
        "    \n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.Q.state_dict(), 'best_model.pth')\n",
        "        print(f'Saved best model at epoch {epoch}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs('evaluation_results/withEmbeddings', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### Generating test set trajectories ####\n",
            "Input shapes: Xtest=(34480, 48), actionbloctest=(34480,), bloctest=(34480,), Y90=(1724,), SOFA=(34480,)\n",
            "Agent-Physician action agreement: 0.03%\n",
            "Training and evaluation complete. Results saved in 'evaluation_results/withEmbeddings' directory.\n"
          ]
        }
      ],
      "source": [
        "# Load best model for testing\n",
        "model.Q.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Prepare test data\n",
        "reward_value = 24  # From reward function\n",
        "beat = [0, 0.6]  # From reward function\n",
        "test_Y90 = np.array([1 if r == -24 else 0 for r in test_data['Reward'][test_data['Done'] == 1]])\n",
        "rec_agent_q, rec_phys_q, rec_agent_a, rec_phys_a, rec_sur, rec_reward_user, rec_agent_q_pro = do_test(\n",
        "    model,\n",
        "    test_data['X'],\n",
        "    test_data['Action'],\n",
        "    test_data['Bloc'],\n",
        "    test_Y90,\n",
        "    test_data['SOFA'],\n",
        "    reward_value,\n",
        "    beat,\n",
        "    'evaluation_results/withEmbeddings'\n",
        ")\n",
        "\n",
        "# Analyze results\n",
        "agreement = np.mean(np.array(rec_agent_a) == np.array(rec_phys_a))\n",
        "print(f\"Agent-Physician action agreement: {agreement:.2%}\")\n",
        "\n",
        "print(\"Training and evaluation complete. Results saved in 'evaluation_results/withEmbeddings' directory.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
