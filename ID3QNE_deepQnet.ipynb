{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMP4B4iF1Pm1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "\n",
        "class DistributionalDQN(nn.Module):\n",
        "    def __init__(self, state_dim, n_actions):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.value_stream = nn.Linear(128, 1)\n",
        "        self.advantage_stream = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.conv(state)\n",
        "        v = self.value_stream(x)\n",
        "        a = self.advantage_stream(x)\n",
        "        q = v + (a - a.mean(dim=1, keepdim=True))\n",
        "        return q\n",
        "\n",
        "class Dist_DQN(object):\n",
        "    def __init__(self, state_dim, n_actions):\n",
        "        self.q_net = DistributionalDQN(state_dim, n_actions)\n",
        "        self.target_net = copy.deepcopy(self.q_net)\n",
        "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=1e-5)\n",
        "        self.gamma = 0.99\n",
        "\n",
        "    def polyak_target_update(self, tau=0.005):\n",
        "        for tp, p in zip(self.target_net.parameters(), self.q_net.parameters()):\n",
        "            tp.data.copy_(tau * p.data + (1 - tau) * tp.data)\n",
        "\n",
        "    def compute_loss(self, batch):\n",
        "        state = torch.tensor(batch['state'], dtype=torch.float32)\n",
        "        next_state = torch.tensor(batch['next_state'], dtype=torch.float32)\n",
        "        action = torch.tensor(batch['action'], dtype=torch.long)\n",
        "        reward = torch.tensor(batch['reward'], dtype=torch.float32)\n",
        "\n",
        "        q_vals = self.q_net(state)\n",
        "        q_val = q_vals.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q = self.target_net(next_state)\n",
        "            max_next = next_q.max(1)[0]\n",
        "            target = reward + self.gamma * max_next\n",
        "\n",
        "        loss = F.mse_loss(q_val, target)\n",
        "        return loss\n",
        "\n",
        "    def train_model(self, batchs, epoch):\n",
        "        self.q_net.train()\n",
        "        loss = self.compute_loss(batchs)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), max_norm=1.0)\n",
        "        self.optimizer.step()\n",
        "        self.polyak_target_update()\n",
        "        return loss.item()\n"
      ]
    }
  ]
}