{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMP4B4iF1Pm1"
      },
      "outputs": [],
      "source": [
        "# ID3QNE_deepQnet.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "\n",
        "class DistributionalDQN(nn.Module):\n",
        "    def __init__(self, state_dim, n_actions):\n",
        "        super(DistributionalDQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 128), nn.ReLU(),\n",
        "        )\n",
        "        self.value = nn.Linear(128, 1)\n",
        "        self.adv   = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.net(x)\n",
        "        v = self.value(h)                  # [B,1]\n",
        "        a = self.adv(h)                    # [B,A]\n",
        "        q = v + (a - a.mean(dim=1, keepdim=True))\n",
        "        return q\n",
        "\n",
        "class Dist_DQN(object):\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.99, lr=1e-5, seed=42):\n",
        "        torch.manual_seed(seed); np.random.seed(seed)\n",
        "        self.q_net = DistributionalDQN(state_dim, n_actions)\n",
        "        self.target_net = copy.deepcopy(self.q_net)\n",
        "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def polyak_target_update(self, tau=0.005):\n",
        "        for tp, p in zip(self.target_net.parameters(), self.q_net.parameters()):\n",
        "            tp.data.copy_(tau * p.data + (1 - tau) * tp.data)\n",
        "\n",
        "    def compute_loss(self, batchs):\n",
        "        state = torch.tensor(batchs['state'], dtype=torch.float32)\n",
        "        next_state = torch.tensor(batchs['next_state'], dtype=torch.float32)\n",
        "        action = torch.tensor(batchs['action'], dtype=torch.long)\n",
        "        reward = torch.tensor(batchs['reward'], dtype=torch.float32)\n",
        "\n",
        "        q_vals = self.q_net(state)                      # [B,A]\n",
        "        q_val = q_vals.gather(1, action.reshape(-1,1)).squeeze(1)  # [B]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q = self.target_net(next_state)        # [B,A]\n",
        "            max_next = next_q.max(1)[0]                 # [B]\n",
        "            target = reward + self.gamma * max_next\n",
        "\n",
        "        loss = F.mse_loss(q_val, target)\n",
        "        return loss\n",
        "\n",
        "    def train_model(self, batchs, epoch):\n",
        "        self.q_net.train()\n",
        "        loss = self.compute_loss(batchs)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "        self.polyak_target_update()\n",
        "        return float(loss.item())\n",
        "\n",
        "    def get_action(self, state_np):\n",
        "        with torch.no_grad():\n",
        "            s = torch.tensor(state_np, dtype=torch.float32).unsqueeze(0)\n",
        "            return int(self.q_net(s).argmax().item())\n"
      ]
    }
  ]
}