{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMP4B4iF1Pm1"
      },
      "outputs": [],
      "source": [
        "# ID3QNE_deepQnet.py\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "\n",
        "# ----- Core PyTorch Dueling DQN Model -----\n",
        "class DistributionalDQN(nn.Module):\n",
        "    def __init__(self, state_dim, n_actions):\n",
        "        super(DistributionalDQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            nn.Linear(128, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.conv(state)\n",
        "        value = self.value_stream(x)\n",
        "        advantage = self.advantage_stream(x)\n",
        "        qvals = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "        return qvals\n",
        "\n",
        "# ----- Wrapper Class for Distributional DQN -----\n",
        "class Dist_DQN(object):\n",
        "    def __init__(self, state_dim, n_actions):\n",
        "        self.q_net = DistributionalDQN(state_dim, n_actions)\n",
        "        self.target_net = copy.deepcopy(self.q_net)\n",
        "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=1e-5)\n",
        "        self.gamma = 0.9\n",
        "\n",
        "    def polyak_target_update(self, tau=0.005):\n",
        "        for target_param, param in zip(self.target_net.parameters(), self.q_net.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "    def compute_loss(self, batch):\n",
        "        state = torch.tensor(batch['state'], dtype=torch.float32)\n",
        "        next_state = torch.tensor(batch['next_state'], dtype=torch.float32)\n",
        "        action = torch.tensor(batch['action'], dtype=torch.long)\n",
        "        reward = torch.tensor(batch['reward'], dtype=torch.float32)\n",
        "\n",
        "        # Input NaN checks\n",
        "        if torch.isnan(state).any():\n",
        "            print(\"NaNs in input state\")\n",
        "            return torch.tensor(float('nan'), requires_grad=True)\n",
        "        if torch.isnan(next_state).any():\n",
        "            print(\"NaNs in input next_state\")\n",
        "            return torch.tensor(float('nan'), requires_grad=True)\n",
        "\n",
        "        q_vals = self.q_net(state)\n",
        "        if torch.isnan(q_vals).any():\n",
        "            print(\"NaN detected in q_vals output from q_net.\")\n",
        "            return torch.tensor(float('nan'), requires_grad=True)\n",
        "\n",
        "        q_val = q_vals.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q_vals = self.target_net(next_state)\n",
        "            if torch.isnan(next_q_vals).any():\n",
        "                print(\"NaN detected in next_q_vals output from target_net.\")\n",
        "                return torch.tensor(float('nan'), requires_grad=True)\n",
        "            max_next_q_val = next_q_vals.max(1)[0]\n",
        "            expected_q_val = reward + self.gamma * max_next_q_val\n",
        "\n",
        "        if torch.isnan(q_val).any() or torch.isnan(expected_q_val).any():\n",
        "            print(\"NaN detected in Q-values used in loss calculation.\")\n",
        "            return torch.tensor(float('nan'), requires_grad=True)\n",
        "\n",
        "        loss = F.mse_loss(q_val, expected_q_val)\n",
        "        return loss\n",
        "\n",
        "    def train_model(self, batchs, epoch):\n",
        "        self.q_net.train()\n",
        "        loss = self.compute_loss(batchs)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), max_norm=1.0)\n",
        "        self.optimizer.step()\n",
        "        self.polyak_target_update()\n",
        "        return loss.item()\n",
        "\n",
        "    def get_action(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            q_vals = self.q_net(state)\n",
        "            return q_vals.argmax().item()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
